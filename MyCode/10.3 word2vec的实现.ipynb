{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.utils.data as Data\n",
    "\n",
    "sys.path.append('../code/')\n",
    "import d2lzh_pytorch as d2l\n",
    "devic=torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# sentenses: 42068'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert 'ptb.train.txt' in os.listdir('../data/ptb/')\n",
    "\n",
    "with open('../data/ptb/ptb.train.txt','r') as f:\n",
    "    lines=f.readlines()\n",
    "    raw_dataset=[st.split() for st in lines]\n",
    "    \n",
    "'# sentenses: %d' % len(raw_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# tokens: 24 ['aer', 'banknote', 'berlitz', 'calloway', 'centrust']\n",
      "# tokens: 15 ['pierre', '<unk>', 'N', 'years', 'old']\n",
      "# tokens: 11 ['mr.', '<unk>', 'is', 'chairman', 'of']\n"
     ]
    }
   ],
   "source": [
    "for st in raw_dataset[:3]:\n",
    "    print('# tokens:',len(st),st[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9858"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter=collections.Counter([tk for st in raw_dataset for tk in st])\n",
    "counter=dict(filter(lambda x:x[1]>=5,counter.items()))\n",
    "len(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# tokens: 376061'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def discard(idx):\n",
    "    return random.uniform(0, 1) < 1 - math.sqrt(\n",
    "        1e-4 / counter[idx_to_token[idx]] * num_tokens)\n",
    "\n",
    "subsampled_dataset = [[tk for tk in st if not discard(tk)] for st in dataset]\n",
    "'# tokens: %d' % sum([len(st) for st in subsampled_dataset]) # '# tokens: 375875'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "887100"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_to_token = [tk for tk,_ in counter.items()]\n",
    "token_to_idx={tk:idx for idx,tk in enumerate(idx_to_token)}\n",
    "dataset=[[token_to_idx[tk] for tk in st if tk in token_to_idx] for st in raw_dataset]\n",
    "num_tokens = sum([len(st) for st in dataset])\n",
    "num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# the:before=50770,after=2098'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compare_counts(token):\n",
    "    return '# %s:before=%d,after=%d' %(token,sum([st.count(token_to_idx[token]) for st in dataset]),\n",
    "            sum([st.count(token_to_idx[token]) for st in subsampled_dataset]))\n",
    "\n",
    "compare_counts('the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# join:before=45,after=45'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_counts('join')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centers_and_contexts(dataset,max_window_size):\n",
    "    centers,contexts=[],[]\n",
    "    for st in dataset:\n",
    "        if len(st)<2:\n",
    "            continue\n",
    "        centers+=st\n",
    "        for center_i in range(len(st)):\n",
    "            window_size=random.randint(1,max_window_size)\n",
    "            indices=list(range(max(0,center_i-window_size),min(len(st),center_i+1+window_size)))\n",
    "            indices.remove(center_i)\n",
    "            contexts.append([st[idx] for idx in indices])\n",
    "    return centers,contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset [[0, 1, 2, 3, 4, 5, 6], [7, 8, 9]]\n",
      "center 0 has contexts [1]\n",
      "center 1 has contexts [0, 2, 3]\n",
      "center 2 has contexts [0, 1, 3, 4]\n",
      "center 3 has contexts [1, 2, 4, 5]\n",
      "center 4 has contexts [3, 5]\n",
      "center 5 has contexts [3, 4, 6]\n",
      "center 6 has contexts [5]\n",
      "center 7 has contexts [8]\n",
      "center 8 has contexts [7, 9]\n",
      "center 9 has contexts [8]\n"
     ]
    }
   ],
   "source": [
    "tiny_dataset = [list(range(7)), list(range(7, 10))]\n",
    "print('dataset',tiny_dataset)\n",
    "for center,context in zip(*get_centers_and_contexts(tiny_dataset,2)):\n",
    "    print('center',center,'has contexts',context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_centers, all_contexts = get_centers_and_contexts(subsampled_dataset, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negatives(all_contexts,sampling_weights,K):\n",
    "    all_negatives,neg_candidates,i=[],[],0\n",
    "    population=list(range(len(sampling_weights)))\n",
    "    for contexts in all_contexts:\n",
    "        negatives=[]\n",
    "        while len(negatives)<len(contexts)*K:\n",
    "            if i==len(neg_candidates):\n",
    "                i,neg_candidates=0,random.choices(population,sampling_weights,k=int(1e5))\n",
    "            neg,i=neg_candidates[i],i+1\n",
    "            \n",
    "            if neg not in set(contexts):\n",
    "                negatives.append(neg)\n",
    "        all_negatives.append(negatives)\n",
    "    return all_negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_weights=[counter[w]**0.75 for w in idx_to_token]\n",
    "all_negatives=get_negatives(all_contexts,sampling_weights,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 3]\n",
      "[[3, 5], [0, 5, 6, 8, 10]]\n",
      "[[53, 4269, 3327, 8436, 6194, 3032, 2091, 9239, 5615, 1607], [482, 1360, 2913, 815, 119, 1742, 3237, 7637, 2726, 7, 1416, 78, 436, 530, 103, 1136, 3154, 3271, 314, 1720, 4752, 84, 828, 2778, 793]]\n"
     ]
    }
   ],
   "source": [
    "print(all_centers[:2])\n",
    "print(all_contexts[:2])\n",
    "print(all_negatives[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,centers,contexts,negatives):\n",
    "        assert len(centers)==len(contexts)==len(negatives)\n",
    "        self.centers=centers\n",
    "        self.contexts=contexts\n",
    "        self.negatives=negatives\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        return (self.centers[index],self.contexts[index],self.negatives[index])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data):\n",
    "    max_len=max(len(c)+len(n) for _,c,n in data)\n",
    "    centers,contexts_negatives,masks,labels=[],[],[],[]\n",
    "    for center,context,negative in data:\n",
    "        cur_len=len(context)+len(negative)\n",
    "        centers+=[center]\n",
    "        contexts_negatives+=[context+negative+[0]* (max_len - cur_len)]\n",
    "        masks += [[1] * cur_len + [0] * (max_len - cur_len)]\n",
    "        labels += [[1] * len(context) + [0] * (max_len - len(context))]\n",
    "    return (torch.tensor(centers).view(-1, 1), torch.tensor(contexts_negatives),\n",
    "            torch.tensor(masks), torch.tensor(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "centers shape: torch.Size([512, 1])\n",
      "contexts_negatives shape: torch.Size([512, 60])\n",
      "masks shape: torch.Size([512, 60])\n",
      "labels shape: torch.Size([512, 60])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "num_workers = 0 if sys.platform.startswith('win32') else 4\n",
    "\n",
    "dataset = MyDataset(all_centers, \n",
    "                    all_contexts, \n",
    "                    all_negatives)\n",
    "data_iter = Data.DataLoader(dataset, batch_size, shuffle=True,\n",
    "                            collate_fn=batchify, \n",
    "                            num_workers=num_workers)\n",
    "for batch in data_iter:\n",
    "    for name, data in zip(['centers', 'contexts_negatives', 'masks',\n",
    "                           'labels'], batch):\n",
    "        print(name, 'shape:', data.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-1.4923, -0.9218, -0.3741, -0.7244],\n",
       "        [-1.0465,  0.8376,  1.7640, -0.0449],\n",
       "        [-0.9930, -0.9658,  0.4450, -0.9252],\n",
       "        [-1.0616, -0.5010, -0.2011, -0.9076],\n",
       "        [ 1.2408, -0.4449, -0.2709, -1.9615],\n",
       "        [ 1.9716, -1.2924,  0.9997,  0.6505],\n",
       "        [-0.7347,  1.4951, -1.1393,  0.7423],\n",
       "        [-1.5388,  0.8921,  0.6642, -0.5877],\n",
       "        [-1.2955, -0.2329,  1.9294, -0.7238],\n",
       "        [ 0.0502, -0.3816, -2.1170, -1.0962],\n",
       "        [-0.9966, -0.2542, -0.1587, -0.6545],\n",
       "        [-1.4093,  0.2142,  0.6566,  1.3255],\n",
       "        [-0.4407, -0.7742,  0.9616,  0.2935],\n",
       "        [-0.5589,  1.6793, -1.3027, -1.3612],\n",
       "        [-1.2708,  0.6169, -0.6204, -1.2055],\n",
       "        [-1.7165,  0.9141,  0.4607,  0.2788],\n",
       "        [ 1.8599, -0.7802,  1.0194,  0.4894],\n",
       "        [ 0.0054,  1.0828, -0.8563,  0.5429],\n",
       "        [-0.3988, -0.3144,  0.4465,  0.5345],\n",
       "        [ 0.0470, -0.4758, -0.2673,  1.0637]], requires_grad=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed=nn.Embedding(num_embeddings=20,embedding_dim=4)\n",
    "embed.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.tensor([[1,2,3],[4,5,6]],dtype=torch.long)\n",
    "embed(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 6])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=torch.ones((2,1,4))\n",
    "Y=torch.ones((2,4,6))\n",
    "torch.bmm(X,Y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skip_gram(center,context_and_negatives,embed_v,embed_u):\n",
    "    v=embed_v(center)\n",
    "    u=embed_u(context_and_negatives)\n",
    "    pred=torch.bmm(v,u.permute(0,2,1))\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmoidBinaryCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SigmoidBinaryCrossEntropyLoss,self).__init__()\n",
    "    def forward(self,inputs,targets,mask=None):\n",
    "        inputs, targets, mask = inputs.float(), targets.float(), mask.float()\n",
    "        res = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\", weight=mask)\n",
    "        return res.mean(dim=1)\n",
    "    \n",
    "loss=SigmoidBinaryCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8740, 1.2100])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred=torch.tensor([[1.5,0.3,-1,2],[1.1,-0.6,2.2,0.4]])\n",
    "label=torch.tensor([[1,0,0,0],[1,1,0,0]])\n",
    "mask=torch.tensor([[1,1,1,1],[1,1,1,0]])\n",
    "loss(pred,label,mask)*mask.shape[1]/mask.float().sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8740\n",
      "1.2100\n"
     ]
    }
   ],
   "source": [
    "def sigmd(x):\n",
    "    return -math.log(1/(1+math.exp(-x)))\n",
    "print('%.4f' % ((sigmd(1.5) + sigmd(-0.3) + sigmd(1) + sigmd(-2)) / 4)) # 注意1-sigmoid(x) = sigmoid(-x)\n",
    "print('%.4f' % ((sigmd(1.1) + sigmd(-0.6) + sigmd(-2.2)) / 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 100\n",
    "net = nn.Sequential(\n",
    "    nn.Embedding(num_embeddings=len(idx_to_token), embedding_dim=embed_size),\n",
    "    nn.Embedding(num_embeddings=len(idx_to_token), embedding_dim=embed_size)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, lr, num_epochs):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(\"train on\", device)\n",
    "    net = net.to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    for epoch in range(num_epochs):\n",
    "        start, l_sum, n = time.time(), 0.0, 0\n",
    "        for batch in data_iter:\n",
    "            center, context_negative, mask, label = [d.to(device) for d in batch]\n",
    "            \n",
    "            pred = skip_gram(center, context_negative, net[0], net[1])\n",
    "\n",
    "            # 使用掩码变量mask来避免填充项对损失函数计算的影响\n",
    "            l = (loss(pred.view(label.shape), label, mask) *\n",
    "                 mask.shape[1] / mask.float().sum(dim=1)).mean() # 一个batch的平均loss\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            l_sum += l.cpu().item()\n",
    "            n += 1\n",
    "        print('epoch %d, loss %.2f, time %.2fs'\n",
    "              % (epoch + 1, l_sum / n, time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on cuda\n",
      "epoch 1, loss 0.25, time 7.99s\n",
      "epoch 2, loss 0.24, time 8.00s\n",
      "epoch 3, loss 0.24, time 8.00s\n",
      "epoch 4, loss 0.24, time 7.99s\n",
      "epoch 5, loss 0.23, time 7.99s\n",
      "epoch 6, loss 0.23, time 7.98s\n",
      "epoch 7, loss 0.23, time 7.99s\n",
      "epoch 8, loss 0.23, time 8.01s\n",
      "epoch 9, loss 0.23, time 7.98s\n",
      "epoch 10, loss 0.23, time 7.99s\n"
     ]
    }
   ],
   "source": [
    "train(net, 0.001, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9858, 100])\n",
      "torch.Size([100])\n",
      "cosine sim=0.481: intel\n",
      "cosine sim=0.451: nec\n",
      "cosine sim=0.428: oliver\n"
     ]
    }
   ],
   "source": [
    "def get_similar_tokens(query_token, k, embed):\n",
    "    W = embed.weight.data\n",
    "    print(W.shape)\n",
    "    x = W[token_to_idx[query_token]]\n",
    "    print(x.shape)\n",
    "    # 添加的1e-9是为了数值稳定性\n",
    "    cos = torch.matmul(W, x) / (torch.sum(W * W, dim=1) * torch.sum(x * x) + 1e-9).sqrt()\n",
    "    _, topk = torch.topk(cos, k=k+1)\n",
    "    topk = topk.cpu().numpy()\n",
    "    for i in topk[1:]:  # 除去输入词\n",
    "        print('cosine sim=%.3f: %s' % (cos[i], (idx_to_token[i])))\n",
    "\n",
    "get_similar_tokens('chip', 3, net[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
